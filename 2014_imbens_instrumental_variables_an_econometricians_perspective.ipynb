{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnbKxRYi2dbxjNIRapckqj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# (Imbens, 2014) Instrumental Variables: An Econometrician's Perspective\n",
        "\n",
        "[Link to paper](https://docs.iza.org/dp8048.pdf)"
      ],
      "metadata": {
        "id": "OhKB7uZXg6GJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "\n",
        "IV methods were initially developed in econometrics in the 1920's. Recent work in the statistics literature has focused on the following:\n",
        "1. binary treatment\n",
        "2. allows for treatment effect heterogeneity\n",
        "3. explicitly uses the potential outcome framework\n",
        "4. includes randomized experimenents with non-compliance, the intention-to-treat or reduced-form estimates are often of greater interest than they are in the traditional econometric simultaneous equations applications.\n",
        "\n"
      ],
      "metadata": {
        "id": "tVaPc6OOg6Ib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Choice versus Chance in Treatment Assignment\n",
        "\n",
        "## 2.1 The Statistics Literature: The Focus on Chance\n",
        "\n",
        "Goes back to Fisher (1925) and Neyman (1923), the randomized experiment (both motivated by agricultural applications where the unit of analysis are plots of land).\n",
        "\n",
        "In modern notation (originating from Rubin (1974)), the unit (plot) level causal effect is a comparison between the two potential outcomes, $Y_i(A)$ and $Y_i(B)$ (e.g., the difference $\\tau_i=Y_i(B)-Y_i(A)$).\n",
        "\n",
        "In a completely randomized experiment with $N$ plots we select $M$ (with $M\\in\\{1,...,N-1\\}$) plots at random to receive fertilizer $B$, with the remaining $N-M$ plots assigned to fertilizer $A$.\n",
        "\n",
        "Thus, the treatment assignment ($W_i\\in\\{A,B\\}$ for plot $i$) is by design independent of the potential outcomes, allowing for the drawing of exact causal inferences.\n",
        "\n",
        "Fisher focused on calculating exact p-values for sharp null hypotheses, typically the null hyp. of no effect whatsoever, $Y_i(A)=Y_i(B)$ for all plots.\n",
        "\n",
        "Neyman focused on developing unbiased estimators for the ATE\n",
        "\n",
        "$$\\sum_i(Y_i(A)-Y_i(B))/N$$\n",
        "\n",
        "and the variance of these estimators.\n",
        "\n",
        "The subsequent literature in statistics focus on extending and generalizing the Fisher and Neyman results that were derived explicitly for randomized experiments to the more general setting of observational studies, with additional information in the form of pretreatment variables or covariates not affected by the treatment.\n",
        "\n",
        "Let $X_i$ denote these covariates. A key assumption is that conditional on these pretreatment variables the assignment to treatment is independent:\n",
        "\n",
        "$$W_i\\perp Y_i(A), Y_i(B) | X_i$$\n",
        "\n",
        "This is known as *unconfoundedness given $X_i$*, also known as *no unmeasured confounders*.\n",
        "\n",
        "This assumption, in combination with the auxilary assumption that for all values of the covariates the probability of being assigned to each level of the treatment is strictly positive (i.e., Positivity assumption), is called *strong ignorability* (Rosenbaum and Rubin, 1984).\n",
        "\n",
        "In the econometrics literature closely related assumptions are related to as *selection-on-observables* (Barnow, Cain and Goldberger (1980)) or *exogeneity*.\n",
        "\n",
        "Under weak ignorability (and thus also under strong ignorability), it is possible to estimate the ATE in large samples (i.e., the ATE is *identified*). Various methods have been proposed, including matching, subclassification, and regression.\n",
        "\n",
        "Robins and coauthors (Robins, 1986; Gill and Robins, 2001; Richardson and Robins, 2013; Van der Laan and Robins, 2003) have extended this approach to settings with sequential treatments.\n",
        "\n",
        "## 2.2 The Econometrics Literature: The Focus on Choice\n",
        "\n",
        "The starting point in the econometrics literature for studying causal effects emphasizes the choices that led to the treatment received.\n",
        "\n",
        "The starting point of economic science is to model these agents as behaving optimally. More specifically this implies that economists think of everyone of these agents as choosnig the level of the treatment to most efficiently pursue their objectives given the constraints they face.\n",
        "\n",
        "## 2.3 Some Examples\n",
        "\n",
        "What is important is that the starting point is different in the two disciplines, and this has led to the development of substantially different methods for causal inference. For example, in the Fisher-Neyman-Rubin tradition, we model:\n",
        "\n",
        "$$\n",
        "Y_i^{\\text{obs}}=Y_i(W_i)=\\begin{cases}\n",
        "      Y_i(h) & \\text{if }W_i=h \\\\\n",
        "      Y_i(f) & \\text{if }w_i=f\n",
        "\\end{cases}$$\n",
        "\n",
        "where we adjust for observed individual characteristics to make $W_i$ effectively random.\n",
        "\n",
        "However, in Roy (1951), he assumes that each individual chooses their treatment optimally, that is,\n",
        "\n",
        "$$\n",
        "W_i=\\begin{cases}\n",
        "      f & \\text{if }Y_i(f)\\geq Y_i(h) \\\\\n",
        "      h & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "This brings to the question of even selection on observables is even possible.\n",
        "\n",
        "\n",
        "## 2.4 Instrumental Variables\n",
        "\n",
        "Instrumental variables methods address the type of selection issues the Roy model raises.\n",
        "\n"
      ],
      "metadata": {
        "id": "fhnMUTLjg6Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. The Classic Example: Supply and Demand\n",
        "\n",
        "The classic example of instrumental variables methods in econometrics is called simultaneous equations.\n",
        "\n",
        "Simultaneous equation models are both at the core of the econometrics canon and at the core of the confusion concerning instrumental variable methods in the statistics literature.\n",
        "\n",
        "This section looks at the supply and demand models that motivated the original research into instrumental variables.\n",
        "\n",
        "Here the *endogeneity*, that is, the violation of unconfoundedness, arises from an equilibrium condition.\n",
        "\n",
        "## 3.1 Discussions in the Statistics Literature\n",
        "\n",
        "## 3.2 The Market for Fish\n",
        "\n",
        "An example of a market for whiting (a particular white fish often used in fish sticks).\n",
        "\n",
        "Graddy collected data on quantities and prices of whiting sold by a particular trader at the Fulton fish market (NYC) on 111 days (1991-1992).\n",
        "\n",
        "Each day during the period covered in this dataset, indexed by $t=1,...,111$, a number of pounds of whiting are sold by this particular trader, denoted by $Q_t^{\\text{obs}}$. The price per pound per day can be represented with $P_t^{\\text{obs}}$.\n",
        "\n",
        "As you can expect, the higher the quantity, the lower the price. (e.g., day 1: 8,058 pounds were sold for an average of 65 cents per pound, and the next day 2,224 pounds were sold for an average of 100 cents).\n",
        "\n",
        "Suppose we are interested in predicting the effect of a tax in this market. To be specific, suppose the government is considering imposing a $100\\times r\\%$ tax (e.g., a 10% tax) on all whiting sold, but before doing so it wishes to predict the average percent change in the quantity sold as a result of the tax.\n",
        "\n",
        "We may formalize that by looking at the average effect on the logarithm of the quantity $\\tau = E[\\ln Q_t(r)-\\ln Q_t(0)]$, where $Q_t(r)$ is the quantity traded on day $t$ if the tax rate were set at $r$.\n",
        "\n",
        "> We look at the diff. of natural log because that approximately is the relative lift.\n",
        "\n",
        "The problem is that in all 111 markets we only observed $Q_t^{\\text{obs}}=Q_t(0)$, thus we can only direclty estimate $E[\\ln Q_t(0)]$ from the data\n",
        "\n",
        "A naive approach is to assume that a atx increase by 10% would simply raise prices by 10%. Through the unconfoundedness assumption that prices can be viewed as set independently of market conditions on a particular day.\n",
        "\n",
        "Formally,\n",
        "\n",
        "$$E[\\ln Q_t(r)|P_t^{\\text{obs}}=p]=E[\\ln Q_t(0)|P_t^{\\text{obs}}=(1+r)\\times p]$$\n",
        "\n",
        "This can then be estimated using a regression.\n",
        "\n",
        "$$\\ln Q_t^{\\text{obs}}=\\alpha^{\\text{ls}}+\\beta^{\\text{ls}}\\times\\ln P_t^{\\text{obs}}+\\epsilon_t$$\n",
        "\n",
        "This is problematic from an Economist's perspective because the unconfoundedness assumption, that prices are independent of the potential outcomes of quantity is independent. In reality, the prices were different *because* the market conditions were different.\n",
        "\n",
        "## 3.3 The Supply of and Demand for Fish\n",
        "\n",
        "So how do economists go about analyzing questions such as this one if not by regressing quantities on prices?\n",
        "\n",
        "Traditionally the demand function is specified parametrically, for example linear in logarithms:\n",
        "\n",
        "$$\\ln Q_t^d(p)=\\alpha^d+\\beta^d\\times\\ln p+\\epsilon_t^d$$\n",
        "\n",
        "where $\\beta^d$ is the price elasticity of demand. This equation is *not* a regression function. It is a *structural equation* and is a model for the potential outcomes.\n",
        "\n",
        "We can normalize the unobserved component $\\epsilon_t^d$ to have expectation zero:\n",
        "\n",
        "$$E[\\ln Q_t^d(p)]=\\alpha^d+\\beta^d\\times\\ln p$$\n",
        "\n",
        "## 3.4 Market Equilibrium\n",
        "\n",
        "## 3.5 The Statistical Demand Curve\n",
        "\n",
        "## 3.6 The Effect of a Tax Increase\n",
        "\n",
        "## 3.7 Identification with Instrumental Variables\n",
        "\n",
        "Need IV's to identify the demand and supply functions.\n",
        "\n",
        "Graddy (1995) assumes that weather conditions at sea on days prior to $t$, denoted $Z_t$, affect supply but not demand. (e.g., high waves and strong winds makes it harder to catch fish, but shouldn't impact whether or not buyers want fish).\n",
        "\n",
        "Formally, the key assumptions are that\n",
        "\n",
        "$$Q_t^d(p)\\perp Z_t\\text{ and }Q_t^s\\not\\perp Z_t$$\n",
        "\n",
        "possibly conditional on covariates.\n",
        "\n",
        "## 3.8 Recent Research on Simultaneous Equation Models\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TqzSRaCwnV8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. A Modern Example: Randomized Experiments with Noncompliance and Heterogenous Treatment Effects\n",
        "\n",
        "The modern literature on instrumental variables methods evolved simultaneously iin the statistics and econometrics literature.\n",
        "\n",
        "In the economic perspective, there were difficulties in establishing point identification (Heckman, 1990), leading to the bounds approach developed by (Manski, 1995).\n",
        "\n",
        "At the same time, statisticians analyzed the complications arising from noncompliance in randomized experiments (Robins, 1989) and the merits of encouragement designs (Zelen, 1979; 1990).\n",
        "\n",
        "By adopting a common framework and notation, these literatures have become closely connected and influenced each other substantially.\n",
        "\n",
        "## 4.1 The McDonald and Tierney (1992) Data\n",
        "\n",
        "The canonical example in this literature is that of a randomized experiment with non-compliance (The McDonald and Tierney from 1992).\n",
        "\n",
        "The two carried out a randomized exp. to evaluate the effect of an influenza vaccination on flue-related hospital visits.\n",
        "\n",
        "Instead of random assignment on receiving the vaccination, the researchers randomly assigned physicians to receive letters reminding them of the upcoming flue season and encouraging them to vaccinate their patients.\n",
        "\n",
        "This is what Zelen (1979) refers to as an *encouragement design*.\n",
        "\n",
        "Let\n",
        "- $Z_i\\in\\{0,1\\}$ be the indicator for the receipient of the letter\n",
        "- $X_i\\in\\{0,1\\}$ be the indicator for the receipt of the vaccination.\n",
        "\n",
        "We can reason that there are 4 potential outcomes. $Y_i(z,x)$.\n",
        "\n",
        "We also know there are two potential outcomes for the receipt of the letter, $X_i(z)$.\n",
        "\n",
        "The treatment actually received is $X_i^{\\text{obs}}=X_i(Z_i)$\n",
        "\n",
        "and the potential outcome corresponding to the assignment and treatment received, $Y_i^{\\text{obs}}=Y_i(Z_i, X_i(Z_i))$\n",
        "\n",
        "Notice that there are 8 possible values of the 3 combinations of $Z_i, X_i, Y_i$.\n",
        "\n",
        "## 4.2 Instrumental Variables Assumptions\n",
        "\n",
        "There are 4 key assumptions underlying IV methods beyond SUTVA (with diff. versions for some of them).\n",
        "\n",
        "**Assumption 1: Random Assignment**\n",
        "\n",
        "The instrument is as good as randomly assigned:\n",
        "\n",
        "$$Z_i\\perp (Y_i(Z_i, X_i), X(Z_i))$$\n",
        "\n",
        "This assumption is often satisfied by design in an encouragement design in the statistics literature, although with observational data, as common in econometrics literature, is more controversial. This can be relaxed by requiring it to hold only within subpopulations defined by covariates, assuming the assignment of the instrument is unconfounded:\n",
        "\n",
        "unconfounded assignment given $X_i$\n",
        "\n",
        "$$Z_i\\perp (Y_i(Z_i, X_i), X(Z_i))|X_i$$\n",
        "\n",
        "Either version of this assumption justifies the causal interpretation of *Intention-to-Treat* (ITT) effects.\n",
        "\n",
        "In many cases, ITT effects are only of limited interest, and thus motivates the consideration of additional assumptions that do allow the researcher to make statements about the causal effects of the treatment of interest.\n",
        "\n",
        "**in order to draw inferences beyond ITT effects, additional assumptions will be used.**\n",
        "\n",
        "The second class of assumptions limits or rules out completely direct effects of the assignment on the outcome, other than through the effect of the assignment on the receipt of the treatment of interest.\n",
        "\n",
        "This is the most critical and most controversial assumption underlying IV methods, sometimes viewed as the defining characteristic of instruments.\n",
        "\n",
        "**Assumption 2: Exclusion-Restriction**\n",
        "\n",
        "$$Y_i(0,x)=Y_i(1,x)\\text{ for }x=0,1,\\text{ for all }i$$\n",
        "\n",
        "There are a lot of diff. weaker versions of this.\n",
        "\n",
        "Imbens and Angrist (1994) combine the above two assumptions by postulating the existence of a pair of potential outcomes $Y_i(x)$ for $x=0,1$ and directly assuming that\n",
        "\n",
        "$$Z_i\\perp (Y_i(0), Y_i(1))$$\n",
        "\n",
        "**Assumption 3: Monotonicity**\n",
        "\n",
        "by Imbens and Angrist (1994), an assumption that is often used, requires that\n",
        "\n",
        "$$X_i(1)\\geq X_i(0)\\text{ for all }i$$\n",
        "\n",
        "Rules out the presence of units who always do the opposite of their assignment (units with $X_i(0)=1$ and $X_i(1)=0$), and is therefore also referred to as the *no-defiance* assumption (Balke and Pearl, 1995).\n",
        "\n",
        "In the case of randomized experiments, often a plausible assumption.\n",
        "\n",
        "Finally, we need\n",
        "\n",
        "**Assumption 4: Relevancy**\n",
        "\n",
        "The instrument needs to be correlated with the instrument\n",
        "\n",
        "$$X_i\\not\\perp Z_i$$\n",
        "\n",
        "In practice, we need the correlation to be substantial in order to draw precise inferences.\n",
        "\n",
        "The modern literature first focused on the inability under these 4 assumptions to identify an ATE. Manski (1990), Balke and Pearl (1995), and Robins (1989) showed that there was some information to derive bounds, but not a point estimate.\n",
        "\n",
        "Another strand of literature starting with Imbens and Angrist (1994), Angrist, Imbens and Rubin (1996) abandoned the effort to do inference for the overall average effect, and focused on subpopulations for which the average effect could be identified (compliers).\n",
        "\n",
        "## 4.3 Point Identification versus Bounds\n",
        "\n",
        "The primary estiamd is usually the ATE or the ATT:\n",
        "\n",
        "$$\\tau=E[Y_i(1)-Y_i(0)]$$\n",
        "$$\\tau_t=E[Y_i(1)-Y_i(0)|X_i=1]$$\n",
        "\n",
        "With only the 4 assumptions:\n",
        "- random assignment\n",
        "- exclusion restrictioni\n",
        "- monotonicity\n",
        "- instrument relevance\n",
        "\n",
        "Robins (1989), Manski (1990), and Balke and Pearl (1995) established that the ATE can not be consistently estiamted even in large samples, in other words, often *not point-identified*.\n",
        "\n",
        "As an alternative to these assumptions, bounds (called *natural bounds*) were developed by Manski (1995-2008), Robins (1989) and Hernan and Robins (2006).\n",
        "\n",
        "## 4.4 Compliance Types\n",
        "\n",
        "Imbens and Angrist (1994), Angrist, Imbens and Rubin (1996) take a diff. approach.\n",
        "\n",
        "They focus on a different average causal effect that *can* be identified.\n",
        "\n",
        "Can also not identify the *proportion* of individuals of each compliance type without additional restrictions. The monotonicity assumption implies that there are no defiers. This means that with random assignment, we can identify the population shares of the remaining 3 compliance types.\n",
        "\n",
        "The proportion of always-takers and never-takers are:\n",
        "\n",
        "$$\\tau_a=\\Pr(T_i=a)=\\Pr (X_i=1|Z_i=0)$$\n",
        "\n",
        "$$\\tau_n=\\Pr(T_i=n)=\\Pr (X_i=0|Z_i=0)$$\n",
        "\n",
        "And thus the proportion of compilers is the remainder:\n",
        "\n",
        "$$\\tau_c=\\Pr(T_i=c)=1-\\tau_a-\\tau_n$$\n",
        "\n",
        "## 4.5 Local Average Treatment Effects\n",
        "\n",
        "So far we have random assignment and monotonicity.\n",
        "\n",
        "By adding exclusion-restriction, Imbens and Angrist (1994) and Angrist, Imbens and Rubin (1996) show that the LATE or Compiler ATE is *identified*:\n",
        "\n",
        "$$\\tau_\\text{late}=E[Y_i(1)-Y_i(0)|T_i=\\text{compiler}]=\\frac{E[Y_i|Z_i=1]-E[Y_i|Z_i=0]}{E[X_i|Z_i=1]-E[X_i|Z_i=0]}$$\n",
        "\n",
        "## 4.6 Do We Care About the Local Average Treatment Effect?\n",
        "\n",
        "The LATE is an unusual estimand. It is an ATE of the treatment for a subpopulation that cannot be identified (i.e., no units whom we know to belong to this population).\n",
        "\n",
        "Thus is controversial:\n",
        "\n",
        "\"I find it hard to make any sense of the LATE\" (Deaton, 2010)\n",
        "\n",
        "\"Most authors in this category do not state whether their focus on a specific stratum is motivated by mathematical convenience, mathematical necessity or a genuine interest in the stratum under analysis\" (Pearl, 2011)\n",
        "\n",
        "However, Imbens states, \"this limitation should be acknowledged, but one should not drop the analysis simply because the original estimand cannot be identified\".\n",
        "\n",
        "Some say to just focus on ITT.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6m0ybUH7nWDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. The SubStantive Content of the Instrumental Variables Assumptions\n",
        "\n",
        "Discuss the substantive content of the 3 key assumptions:\n",
        "- random assignment\n",
        "- exclusion restriction\n",
        "- monotonicity assumption\n",
        "\n",
        "## 5.1 Unconfoundedness of the Instrument\n",
        "\n",
        "i.e., random assignment\n",
        "\n",
        "In many applications, satisfied by design (because the instrument is physically randomized).\n",
        "\n",
        "## 5.2 The Exclusion Restriction\n",
        "\n",
        "Most critical and typically most controversial assumption underlying instrumental variables methods.\n",
        "\n",
        "## 5.3 Monotonicity\n",
        "\n",
        "i.e., no-defiers assumption. Least controversial.\n",
        "\n",
        "\"Often, but not always reasonable\" (Robins, 1989).\n",
        "\n",
        "In one-sided noncompliance, those assigned to the control are effectively embargoed from receiving treatment, thus monotonicity is automatically satisfied.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PKNMaIijFcTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Extentions and Generalizations\n",
        "\n",
        "## 7.1 Model-based Approaches to Estimation and Inference\n",
        "\n",
        "Traditionally instrumental variable analyses relied on linear regression methods.\n",
        "\n",
        "Additional explanatory variables are incorporated linearly in the regression function.\n",
        "\n",
        "The recent work in the statistics literature has explore more flexible approaches to including covariates.\n",
        "\n",
        "Often involves modeling the conditional distribution of the endogenous regressor given the instruments and the exogenous variables.\n",
        "\n",
        "## 7.2 Principal Stratification\n",
        "\n",
        "## 7.3 Randomization Inference with IV'S\n",
        "\n",
        "## 7.5 Weak Instruments\n",
        "\n",
        "The weak isntrument literature is concerned with the construction of confidence intervals, especially after a study by Angrist and Krueger (1991).\n",
        "\n",
        "## 7.6 Many Instruments\n",
        "\n",
        "## 7.7 Proxies for Instruments\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HSliOsiDFcVd"
      }
    }
  ]
}